{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport transformers\\nfrom transformers import (\\n    AutoConfig,\\n    AutoModelForMultipleChoice,\\n    AutoTokenizer,\\n    HfArgumentParser,\\n    Trainer,\\n    TrainingArguments,\\n    default_data_collator,\\n    set_seed,\\n)\\nfrom transformers.file_utils import PaddingStrategy\\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase\\nfrom transformers.trainer_utils import get_last_checkpoint\\nfrom transformers.utils import check_min_version\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "'''\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMultipleChoice,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import PaddingStrategy\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join\n",
    "import jsonlines\n",
    "def parse(self, response):\n",
    "    jsonresponse=json.loads(response.text)\n",
    "    with jsonlines.open('output.jsonl',mode='a') as writer:\n",
    "        writer.write(jsonresponse)\n",
    "\n",
    "def get_dataset(file_path):\n",
    "    dataset = []\n",
    "    with open(file_path, \"r+\", encoding=\"utf8\") as f:\n",
    "        for item in jsonlines.Reader(f):\n",
    "            dataset.append(item)\n",
    "    return dataset\n",
    "\n",
    "def save_dataset(dataset, file_path):\n",
    "    with open(file_path, \"w+\", encoding=\"utf8\") as f:\n",
    "        for item in dataset:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "train_data = get_dataset(join('data','train.jsonl'))\n",
    "val_data = get_dataset(join('data','valid.jsonl'))\n",
    "test_data = get_dataset(join('data','test_public.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2720/2720 [00:01<00:00, 2088.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from jiayan import PMIEntropyLexiconConstructor\n",
    "from jiayan import load_lm\n",
    "from jiayan import CharHMMTokenizer\n",
    "\n",
    "lm = load_lm(join('jiayan_models','jiayan.klm'))\n",
    "tokenizer = CharHMMTokenizer(lm)\n",
    "\n",
    "for data in tqdm(test_data):\n",
    "    choices = data['choices']\n",
    "    data['split_choices'] = []\n",
    "    for choice in choices:\n",
    "        tokens = list(tokenizer.tokenize(choice))\n",
    "        data['split_choices'].append(tokens)\n",
    "save_dataset(test_data, join('data','test_split.jsonl'))\n",
    "assert 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2720/2720 [00:00<00:00, 47747.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "train_data = get_dataset(join('data','train.jsonl'))\n",
    "val_data = get_dataset(join('data','valid.jsonl'))\n",
    "test_data = get_dataset(join('data','test_public.jsonl'))\n",
    "\n",
    "def split_verse(verse):\n",
    "    verse_list = re.split('[，。]', verse)\n",
    "    entry_list = []\n",
    "    for verse in verse_list:\n",
    "        if len(verse) == 5:\n",
    "            entry_list.append(verse[:2])\n",
    "            entry_list.append(verse[2:])\n",
    "        elif len(verse) == 7:\n",
    "            entry_list.append(verse[:2])\n",
    "            entry_list.append(verse[2:4])\n",
    "            entry_list.append(verse[4:])\n",
    "            entry_list.append(verse[:4])\n",
    "        else:\n",
    "            tokens = list(tokenizer.tokenize(verse))\n",
    "            entry = ''\n",
    "            for token in tokens:\n",
    "                if len(token) > 1:\n",
    "                    entry_list.append(entry)\n",
    "                    entry = ''\n",
    "                    entry_list.append(token)\n",
    "                else:\n",
    "                    entry += token\n",
    "            if entry != '':\n",
    "                entry_list.append(entry)\n",
    "    return entry_list\n",
    "\n",
    "\n",
    "for data in tqdm(test_data):\n",
    "    choices = data['choices']\n",
    "    data['split_choices'] = []\n",
    "    for choice in choices:\n",
    "        tokens = split_verse(choice)\n",
    "        data['split_choices'].append(tokens)\n",
    "save_dataset(test_data, join('data','test_split.jsonl'))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edbfbff82cc5a40b1a92271e8866fe127716c5ba884a27844aa7aa39c9a2b64d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ccpm': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
